# ============================================================
# Aluminum Cold Roll â€“ CatBoost with Feature Engineering (Kaggle)
# ============================================================

# --- 1. Setup ------------------------------------------------

set.seed(123)

library(tidyverse)
library(pROC)
library(catboost)
library(lightgbm)  # optional, not used below


# --- 2. Load data --------------------------------------------

train <- read.csv("/kaggle/input/f-2025-101-c-final-project-x/aluminum_coldRoll_train.csv")
test  <- read.csv("/kaggle/input/f-2025-101-c-final-project-x/aluminum_coldRoll_testNoY.csv")

str(train)


# --- 3. Basic preprocessing (factors, design matrices) -------

# Categorical variables
cat_vars <- c(
  "alloy",
  "cutTemp",
  "rollTemp",
  "topEdgeMicroChipping",
  "blockSource",
  "machineRestart"
)

# Convert to factors in train & test
for (v in cat_vars) {
  if (v %in% names(train)) train[[v]] <- as.factor(train[[v]])
  if (v %in% names(test))  test[[v]]  <- as.factor(test[[v]])
}

# Align levels between train & test
for (v in cat_vars) {
  if (v %in% names(train) && v %in% names(test)) {
    test[[v]] <- factor(test[[v]], levels = levels(train[[v]]))
  }
}

# Target as numeric 0/1
y_train <- train$y_passXtremeDurability
if (!is.numeric(y_train)) {
  y_train <- as.numeric(as.character(y_train))
}

# Feature data frames (no ID, no target)
X_train_df <- train %>%
  dplyr::select(-ID, -y_passXtremeDurability)

X_test_df <- test %>%
  dplyr::select(-ID)

# (Optional) simple train/validation split
set.seed(123)
n          <- nrow(X_train_df)
train_size <- floor(0.8 * n)
idx        <- sample(seq_len(n), size = train_size)
# X_tr  <- X_train_df[idx, , drop = FALSE]
# y_tr  <- y_train[idx]
# X_val <- X_train_df[-idx, , drop = FALSE]
# y_val <- y_train[-idx]


# --- 4. Feature engineering ----------------------------------

fe_engineer <- function(df, cat_vars) {
  out <- df
  num_cols <- setdiff(names(df), cat_vars)
  
  for (col in num_cols) {
    x <- out[[col]]
    if (!is.numeric(x)) next
    
    # squared term
    out[[paste0(col, "_sq")]] <- x^2
    
    # log1p term for strictly positive features
    if (all(x > 0, na.rm = TRUE)) {
      out[[paste0(col, "_log1p")]] <- log1p(x)
    }
  }
  
  out
}

# Apply FE to train/test feature frames
X_train_fe <- fe_engineer(X_train_df, cat_vars)
X_test_fe  <- fe_engineer(X_test_df,  cat_vars)

# Categorical feature indices for CatBoost (0-based) in FE matrices
cat_feature_indices_fe <- which(names(X_train_fe) %in% cat_vars) - 1L

cat("Categorical feature indices (0-based):\n")
print(cat_feature_indices_fe)


# --- 5. 5-fold stratified CV on engineered data --------------

# Full training pool with engineered features
full_pool_fe <- catboost.load_pool(
  data         = X_train_fe,
  label        = y_train,
  cat_features = cat_feature_indices_fe
)

# Final tuned hyperparameters
final_params <- list(
  loss_function     = "Logloss",
  eval_metric       = "Logloss",
  iterations        = 3000,
  learning_rate     = 0.08,
  depth             = 4,
  l2_leaf_reg       = 3,
  random_seed       = 123,
  od_type           = "Iter",
  od_wait           = 70,
  use_be_
